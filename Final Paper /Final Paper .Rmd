---
title: Webscrabing Doctors' Information from the United States Immigration website as a first step to track down doctors and facilities that have been harassing immigrants
author:
  - name: Rutendo Madziwo
    email: rmadziwo@smith.edu
    affiliation: Smith College

  - name: Maggie Szlosek
    email: mszlosek@smith.edu
    affiliation: Smith College 
  - name: Rachel LaFlamme
    email: rlaflamme@smith.edu
    affiliation: Smith College

abstract: Ru's working on it. 
A paper of no more than 20 pages reporting the results of your project that includes the following:
The research question(s)
Background/significance of the research
The methods used to obtain and analyze the data
The results of the analysis (tables, charts, graphs, significance, confidence intervals, descriptive text)
A discussion of the research, the limitations of the current research, reasonableness of any assumptions made, possibilities of future work/studies that should be conducted, etc.
References should be listed at the end of the report and do not count against the 20 page limit.
In addition to the 20 pages with the main content you should have a separate title page which includes: The title of the project and a one-paragraph abstract of the entire project with recommended length of no more than 150 words.
  
author_summary: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

bibliography: mybibfile.bib
output: rticles::plos_article
csl: plos.csl
---

_Text based on plos sample manuscript, see [http://journals.plos.org/ploscompbiol/s/latex](http://journals.plos.org/ploscompbiol/s/latex)_

# Introduction

#Maggie's 
The research question(s)
Background/significance of the research

#Method 
##Tools and Packages Used: 
**Docker**
Docker is a platform to develop, deploy, and run applications inside containers[insert citation] and was very useful in our project as we needed to interact with our web-browser. We were able to virtually interact with the USCIS website by connecting to a port in Docker and opening Chrome. This virtual interaction allowed us to control what was happening on the website at a given time. 

We also used RSelenium - a package in R which helps one connect to a Selenium server. This server in turn connected to the Chrome web browser and hence allowed us to automate our webscraping experience. 

To actually scrape our data, we used rvest and xml2 in R. We also used purrr for accuratelt mapping our datasets together and dplyr and tidyverse for text-processing and cleaning our data. 

#Process
Initially, Docker was installed before installing and loading the needed R Packages. The command `docker run -d -p 4445:4444 selenium/standalone-chrome` was then run in the R Terminal after we had installed our packages. This command sets up the virtual Chrome container to enable interaction with the Chrome web browser via Selenium. In order to check if Docker is running, one can type in `docker ps`. We eventually open the browser using RSelenium commands before scraping our data. 

#better? explanation 

The advantage of using RSelenium and Docker hand in hand were that it enabled us to see what was happening to the website as we were working on it. RSelenium is responsible not only for opening and closing the browser, but it allowed us to automate most of the things we needed to do on the website. This was especially useful as our website had no endpoint urls and hence could not rely on more traditional web scraping methods. In addition, it made the process of scraping the data faster as one can simply allow the code to run and scrape multiple pages without needing to manually click the specific website. 

Using RSelenium, we opened the port we created in the Docker container and opened a virtual Chrome browser. We automatically navigated to our web page by entering the website url in our code. Our webpage has a search bar, so again we took advantage of the `findElement()` command in the RSelenium and entered the desired zipcode in the search bar. We also used this command to find the `Next` button in our web page and automatically click to the next pages. At the end of the session, we used RSelenium to close and ensure that we no garbage remaining in order to save memory. 

While RSelenium was responsible for most of the web manouvering, the package we used for scraping the data from each of the pages was `rvest `. This package makes harvesting data from a website easy as it can find specific html nodes, and their children. It also allows one to use both XPaths and CSS selectors so though we eventually stuck to using basic elements, we were not limited to one option. As a side note, we chose to use CSS selectors for web navigation with the RSelenium package. 

We created a function to scrape this data and took advantage of the purrr package in R to map all our 



#Maggie's 
A discussion of the research, the limitations of the current research, reasonableness of any assumptions made, possibilities of future work/studies that should be conducted, etc.


#Ru's

The title of the project and a one-paragraph abstract of the entire project with recommended length of no more than 150 words.



Here are two sample references: @Feynman1963118 [@Dirac1953888].

# References {#references .unnumbered}
